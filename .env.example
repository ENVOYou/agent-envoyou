# .env.example for Agent Envoyou
# Envoyou Multi-Provider AI Agent System
# Environment Configuration for Multi-Agent Fullstack Development

# =============================================================================
# AI PROVIDER CONFIGURATION
# Uncomment ONE provider configuration below (comment out others)
# =============================================================================

# Provider 1: Google AI (Gemini) - Default
AI_PROVIDER=GOOGLE
# Get API key: https://aistudio.google.com/apikey
GOOGLE_GENAI_USE_VERTEXAI=False
GOOGLE_API_KEY=YOUR_GOOGLE_API_KEY_HERE

# Provider 2: Google Cloud Vertex AI
# AI_PROVIDER=GOOGLE_CLOUD
# GOOGLE_GENAI_USE_VERTEXAI=True
# GOOGLE_CLOUD_PROJECT=YOUR_PROJECT_ID
# GOOGLE_CLOUD_LOCATION=LOCATION

# Provider 3: OpenAI (GPT-4, GPT-3.5)
# AI_PROVIDER=OPENAI
# OPENAI_API_KEY=YOUR_OPENAI_API_KEY_HERE
# OPENAI_BASE_URL=https://api.openai.com/v1

# Provider 4: Anthropic (Claude)
# AI_PROVIDER=ANTHROPIC
# ANTHROPIC_API_KEY=YOUR_ANTHROPIC_API_KEY_HERE
# ANTHROPIC_BASE_URL=https://api.anthropic.com/v1

# Provider 5: xAI (Grok)
# AI_PROVIDER=XAI
# XAI_API_KEY=YOUR_XAI_API_KEY_HERE
# XAI_BASE_URL=https://api.x.ai/v1

# Provider 6: OpenRouter (Multiple models)
# AI_PROVIDER=OPENROUTER
# OPENROUTER_API_KEY=YOUR_OPENROUTER_API_KEY_HERE
# OPENROUTER_BASE_URL=https://openrouter.ai/api/v1

# Provider 7: Local/Ollama
# AI_PROVIDER=OLLAMA
# OLLAMA_HOST=http://localhost:11434
# OLLAMA_MODEL=llama2

# =============================================================================
# MODEL CONFIGURATION
# =============================================================================

# Model selection based on provider
# GOOGLE: gemini-2.5-pro-latest, gemini-2.5-flash
# OPENAI: gpt-4o, gpt-4o-mini, gpt-4-turbo
# ANTHROPIC: claude-3-5-sonnet-20241022, claude-3-opus-20240229
# XAI: grok-beta, grok-vision-beta
# OPENROUTER: Any model available on OpenRouter
# OLLAMA: Any model installed locally

# Primary model for complex tasks
PRIMARY_MODEL=gemini-2.5-pro-latest
# Secondary model for faster responses
FAST_MODEL=gemini-2.5-flash

# Model parameters (common across providers)
MODEL_TEMPERATURE=0.7
MODEL_MAX_TOKENS=4096
MODEL_TOP_P=0.9
MODEL_FREQUENCY_PENALTY=0.0
MODEL_PRESENCE_PENALTY=0.0

# =============================================================================
# AGENT SPECIALIZATION
# =============================================================================

# (Optional) Override models for specialized agents
# Frontend development agents
FRONTEND_COMPLEXITY_MODEL=gpt-4o
FRONTEND_SIMPLE_MODEL=gpt-4o-mini

# Backend development agents  
BACKEND_COMPLEXITY_MODEL=claude-3-5-sonnet-20241022
BACKEND_SIMPLE_MODEL=claude-3-haiku-20240307

# Code review and refactoring agents
CODE_REVIEW_MODEL=grok-beta
CODE_REFACTOR_MODEL=gemini-2.5-pro-latest

# =============================================================================
# DEVELOPMENT SETTINGS
# =============================================================================

# ADK Configuration
ADK_ENVIRONMENT=development
ADK_LOG_LEVEL=INFO
ADK_DEBUG=True

# Provider fallback settings
ENABLE_PROVIDER_FALLBACK=True
PROVIDER_TIMEOUT_SECONDS=60
PROVIDER_RETRY_ATTEMPTS=3

# =============================================================================
# PROJECT INFORMATION
# =============================================================================

PROJECT_NAME=multi_provider_fullstack_agent
PROJECT_VERSION=2.0.0
PROJECT_DESCRIPTION=Multi-Provider AI Agent System for Fullstack Development

# =============================================================================
# SECURITY SETTINGS
# =============================================================================

# API Security
API_RATE_LIMIT=100
API_RATE_WINDOW=60
API_KEY_ROTATION_ENABLED=True

# Provider security
PROVIDER_ENCRYPTION=True
SECURE_CREDENTIAL_STORAGE=True

# =============================================================================
# MONITORING & LOGGING
# =============================================================================

# Provider monitoring
PROVIDER_MONITORING_ENABLED=True
PROVIDER_PERFORMANCE_TRACKING=True
PROVIDER_COST_TRACKING=True

# Logging
VERBOSE_LOGGING=True
LOG_TO_FILE=True
LOG_FILE_PATH=./logs/multi_provider_agents.log
LOG_ROTATION=DAILY

# Performance monitoring
PERFORMANCE_MONITORING=True
METRICS_ENABLED=True
METRICS_ENDPOINT=/metrics

# =============================================================================
# PROVIDER SPECIFIC SETTINGS
# =============================================================================

# OpenAI specific
OPENAI_ORG_ID=YOUR_ORG_ID_HERE

# Anthropic specific
ANTHROPIC_VERSION=2023-06-01

# OpenRouter specific
OPENROUTER_SITE_URL=http://localhost:3000
OPENROUTER_REFERRER=http://localhost:3000

# =============================================================================
# USAGE NOTES
# =============================================================================
# 1. Choose ONE AI provider by uncommenting its configuration
# 2. Comment out all other provider configurations to avoid conflicts
# 3. Replace placeholder API keys with your actual keys
# 4. Each provider requires specific environment setup
# 5. Models can be swapped based on task complexity
# 6. Fallback mechanisms ensure reliability across providers
# 
# For production:
# - Use Google Cloud Vertex AI or OpenAI
# - Enable provider monitoring and cost tracking
# - Implement proper API key rotation
# - Set up comprehensive logging and monitoring